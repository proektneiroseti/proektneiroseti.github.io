<!DOCTYPE html>
<html lang="ru">
<head>
	<meta charset="UTF-8">
	<link href="a3.css" rel="stylesheet" type="text/css">
	<title>Архитектуры нейросетей</title>
</head>
<body>

	<header class="header">
		<div class="container">
			
			<nav class="nav">
				<a class="button__g" href="index.html">Главная</a>
				<a class="button" href="areas of use.html">Области<br>применения</a>
				<a class="button" href="architectures.html">Архитектуры<br>нейросетей</a>
				<a class="button" href="how create.html">Как создать<br>нейросеть</a>
				<a class="button__g" href="about.html">О проекте</a>
			</nav>
			
		</div>
	</header>
	<p>
	<ul>
		<b>Многослойный перцептрон</b><br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Многослойный перцептрон состоит из 3 или более слоев. Он использует нелинейную функцию активации,<br>
		часто тангенциальную или логистическую, которая позволяет классифицировать линейно неразделимые данные.<br>
		Каждый узел в слое соединен с каждый узлом в последующем слое, что делает сеть полностью связанной.<br>
		Такая архитектура находит применение в задачах распознавания речи и машинном переводе.<br><br><br><br>

		<b>Сверточная нейронная сеть</b><br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Сверточная нейронная сеть (Convolutional neural network, CNN) содержит один или более объединенных или<br>
		соединенных сверточных слоев. CNN использует вариацию многослойного перцептрона, рассмотренного выше.<br>
		Сверточные слои используют  операцию свертки для входных данных и передают результат в следующий слой. Эта<br>
		операция позволяет сети быть глубже с меньшим количеством параметров.<br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Сверточные сети показывают выдающиеся результаты в приложениях к картинкам и речи. В статье Convolutional<br>
		Neural Networks for Sentence Classification автор описывает процесс и результаты задач классификации текста с помощью CNN.<br>
		В работе представлена модель на основе word2vec, которая проводит эксперименты, тестируется на нескольких бенчмарках<br>
		и демонстрирует блестящие результаты.<br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В работе Text Understanding from Scratch авторы показывают, что сверточная сеть достигает выдающихся результатов<br>
		даже без знания слов, фраз предложений и любых других синтаксических или семантических структур присущих человеческому<br>
		языку. Семантический разбор, поиск парафраз, распознавание речи — тоже приложения CNN.<br><br><br><br>
		
		<b>Рекурсивная нейронная сеть</b><br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Рекурсивная нейронная сеть — тип глубокой нейронной сети, сформированный при применении одних и тех же наборов<br>
		весов рекурсивно над структурой, чтобы сделать скалярное или структурированное предсказание над входной структурой переменного<br>
		размера через активацию структуры в топологическом порядке. В простейшей архитектуре нелинейность, такая как тангенциальная<br>
		функция активации, и матрица весов, разделяемая всей сетью, используются для объединения узлов в родительские объекты.<br><br><br><br>

		<b>Рекуррентная нейронная сеть</b><br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Рекуррентная нейронная сеть, в отличие от прямой нейронной сети, является вариантом рекурсивной ИНС, в которой<br>
		связи между нейронами — направленные циклы. Последнее означает, что выходная информация зависит не только от текущего входа, но<br>
		также от состояний нейрона на предыдущем шаге. Такая память позволяет пользователям решать задачи NLP: распознание рукописного<br>
		текста или речи. В статье Natural Language Generation, Paraphrasing and Summarization of User Reviews with Recurrent Neural Networks<br>
		авторы показывают модель рекуррентной сети, которая генерирует новые предложения и краткое содержание текстового документа.<br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Siwei Lai, Liheng Xu, Kang Liu, и Jun Zhao в своей работе Recurrent Convolutional Neural Networks for Text Classification<br>
		создали рекуррентную сверточную нейросеть для классификации текста без рукотворных признаков. Модель сравнивается с существующими<br>
		методами классификации текста — Bag of Words, Bigrams + LR, SVM, LDA, Tree Kernels, рекурсивными и сверточными сетями. Описанная<br>
		модель превосходит по качеству традиционные методы для всех используемых датасетов.<br><br><br><br>

		<b>LSTM</b><br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Сеть долгой краткосрочной памяти (Long Short-Term Memory, LSTM) — разновидность архитектуры рекуррентной нейросети,<br>
		созданная для более точного моделирования временных последовательностей и их долгосрочных зависимостей, чем традиционная рекуррентная<br>
		сеть. LSTM-сеть не использует функцию активации в рекуррентных компонентах, сохраненные значения не модифицируются, а градиент не<br>
		стремится исчезнуть во время тренировки. Часто LSTM применяется в блоках по несколько элементов. Эти блоки состоят из 3 или 4 затворов<br>
		(например, входного, выходного и гейта забывания), которые контролируют построение информационного потока по логистической функции.<br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling авторы показывают<br>
		архитектуру глубокой LSTM рекуррентной сети, которая достигает хороших результатов для крупномасштабного акустического моделирования.<br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В работе Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network представлена модель<br>
		для автоматической морфологической разметки. Модель показывает точность 97.4 % в задаче разметки. Apple, Amazon, Google, Microsoft и<br>
		другие компании внедрили в продукты LSTM-сети как фундаментальный элемент.<br><br><br><br>

		<b>Sequence-to-sequence модель</b><br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Часто Sequence-to-sequence модели состоят из двух рекуррентных сетей: кодировщика, который обрабатывает входные данные,<br>
		и декодера, который осуществляет вывод.<br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Читайте: Оценка глубины на изображении при помощи Encoder-Decoder сетей<br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequence-to-Sequence модели часто используются в вопросно-ответных системах, чат-ботах и машинном переводе. Такие многослойные<br>
		ячейки успешно использовались в sequence-to-sequence моделях для перевода в статье Sequence to Sequence Learning with Neural Networks study.<br><br>		
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В Paraphrase Detection Using Recursive Autoencoder представлена новая рекурсивная архитектура автокодировщика, в которой<br>
		представления — вектора в n-мерном семантическом пространстве, где фразы с похожими значением близки друг к другу.<br><br><br><br>

		<b>Неглубокие (shallow) нейронные сети</b><br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Неглубокие модели, как и глубокие нейронные сети, тоже популярные и полезные инструменты. Например, word2vec — группа неглубоких<br>
		двухслойных моделей, которая используется для создания векторных представлений слов (word embeddings). Представленная в Efficient<br>
		Estimation of Word Representations in Vector Space, word2vec принимает на входе большой корпус текста и создает векторное пространство.<br>
		Каждому слову в этом корпусе приписывается соответствующий вектор в этом пространстве. Отличительное свойство — слова из общих текстов<br>
		в корпусе расположены близко друг к другу в векторном пространстве.<br><br>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В статье описаны архитектуры нейронных сетей: глубокий многослойный перцептрон, сверточная, рекурсивная, рекуррентная сети, нейросети<br>
		долгой краткосрочной памяти, sequence-to-sequence модели и неглубокие (shallow) сети, word2vec для векторных представлений слов. Кроме<br>
		того, было показано, как функционируют эти сети, и как различные модели справляются с задачами обработки естественного языка. Также отмечено,<br>
		что сверточные нейронные сети в основном используются для задач классификации текста, в то время как рекуррентные сети хорошо работают с<br>
		воспроизведением естественного языка или машинным переводом. В следующих части серии будут описаны существующие инструменты и библиотеки<br>
		для реализации описанных типов нейросетей.

	</ul> 

	</p>
	<div class="footer">
		<p align="center">
			<br>
			<a class="footer__s" href="index.html">Главная</a>
			<a class="footer__s" href="areas of use.html">Области применения</a>
			<a class="footer__s" href="architectures.html">Архитектуры нейросетей</a>
			<a class="footer__s" href="how create.html">Как создать нейросеть</a>
			<a class="footer__s" href="about.html">О проекте</a>
		</p>
	</div>
</body>
</html>